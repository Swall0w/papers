# [What’s your ML Test Score? A rubric for ML production systems](https://www.eecs.tufts.edu/~dsculley/papers/ml_test_score.pdf)
Eric Breck, Shanqing Cai, Eric Nielsen, Michael Salib, D. Sculley

Google
## どんなもの？(コントリビューション)
機械学習のモデルを含めたサービスを行う際のテスト指標を提示

## 先行研究と比べてどこがすごい？
ソフトウェアのサービスをどう行うかなどの論文は提示されているが，
機械学習のモデルを含めたサービスでのテストなどの指針はまだないため，そこを提示した．

## 技術や手法の肝はどこ？
* 特徴量とデータ
* モデル開発
* 機械学習インフラ
* 機械学習周りの監視
の4つの点からテスト方法を提示した．

### Tests for Features and Data
* Test that distributions of each feature match your experiments.
* Test the relationship between each feature and the target, and the pairwise correlations between individual singnals.
* Test the cost of each features
* Test that a model does not contain any features that have been maunally determined as unsuitable for use.
* Test taht your system maintains privacy controls across its entire data pipeline.
* Test the calendar time needed to develop and add a new feature to the production model.
* Test all code that creats input features, both in trainig and serving.

### Tests for Model Development
* Test that every model specification undergoes a code review and is checked in to a repository.
* Test the relationship between offline proxy metrics and the acutural impact metrics.
* Test the impact of each tunable hyperparameter.
* Test the effect of model staleness.
* Test against a simpler model as a baseline.
* Test model quality on important data slices.
* Test the model for implicit bias.

### Tests for ML Infrastructure
* Test the reproducibility of training
* Unit test model specification code.
* Integration test the full ML pipeline.
* Test model quality before attempting to serve it.
* Test that a single example or training batch can be sent to the model, and changes to internal state can be observed from training through to prediction.
* Test models via a canary process before they enter production serving enviroments
* Test how quickly and safely a model can be rolled back to a previous serving version.

### Monitoring Tests for ML
* Test for upstream instability in features, both in training and serving
* Test that data invariants hold in training and serving inputs.
* Test that your training and serving feature s compute the same values
* Test for model staleness.
* Test for NaNs or infinities appearing in your model during training or serving.
* Tetst for dramatic or slow-leak regressions in training speed, serving latency, throughout, or RAM usage.
* Test for regressions in prediction quality on served data.

## どうやって有効だと検証したか？
Googleサービス内での経験をもとにまとめた．

## 議論はある？
## 次に読むべき論文は？
* Testing scintific software: A systematic literature review.
* An approach to software testing of machine learning applications
