# [SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and &lt;0.5MB model size](https://arxiv.org/abs/1602.07360)
Iandola, Forrest N., Han, Song, Moskewicz, Matthew W., Ashraf, Khalid, Dally, William J., Keutzer, Kurt

DeepScale, UC Berkeley, Stanford University

## どんなもの？
精度を落とさず、CNNのモデルを圧縮した。(AlexNet性能)
また、どうすればいいモデルの形を探索できるかの直感を説明

## 先行研究と比べてどこがすごい？
精度変わらずに圧縮して、最高0.47MB(32bit Alexnetと比較して510倍)まで圧縮した。
特に圧縮していない場合、4.8MB程度で、それまでの最高が6.9MBだった。

## 技術や手法の肝はどこ？
* 3x3フィルタを1x1に変更する
* 3x3フィルタに入力されるチャネル数を減らす
* 遅い段階で、ダウンサンプリング（ストライド2を含め)し始める。（そのほうが広い領域の活性マップが得られる)

## どうやって有効だと検証したか？
ImageNetで学習＆AlexNet及び、他の圧縮手法と比較

## 議論はある？
1x1を多く多用しているが、周辺情報が活用できているのが驚き。遅い段階でのダウンサンプリングがどの程度影響しているのかが重要。

## 次に読むべき論文は？
* Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding
