# [Effective Approaches to Attention-based Neural Machine Translation](https://arxiv.org/abs/1508.04025)
Minh-Thang Luong, Hieu Pham, Christopher D. Manning

Stanford University
## どんなもの？
Attentionメカニズムに基づいた効率的な機械翻訳アルゴリズムの提案

## 先行研究と比べてどこがすごい？
* 2つのアテンションメカニズムに基づくメカニズム提案
*  WMT’15データセットでSOTA

## 技術や手法の肝はどこ？
### Attention Mechanisms
* Global attentional model
全体のソースに対し，推論するときに，その時の隠れ変数と各要素の加重平均をコンテキストベクターとするアテンションメカニズム

* Local attentional model
推論するときに，どこを見ればいいかという位置推論をし，その周辺単語からコンテキストベクターを作成し，推論するメカニズム

## Else
* Input-feeding Approach
推論時にアテンションレイヤーから出てきた結果を再代入し，次単語を推論

## どうやって有効だと検証したか？
WMT'14, 15データセットで学習，評価

ablation studyではないが，それぞれの機能を段階的に追加して，性能が上がることを示した．

## 議論はある？
* コンテキストベクター作成時に情報をまとめる時，行列のconcatenateだと精度が悪くなったがなぜか．
* Alignment visualization がいまいち理解できていない

## 次に読むべき論文は？
* Neural machine translation by jointly learning to align and translate
* DRAW: A recurrent neural network for image generation
* Addressing the rare word problem in neural machine translation
