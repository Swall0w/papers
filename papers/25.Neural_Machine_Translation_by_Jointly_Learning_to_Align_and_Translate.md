# [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473)
Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio

* Jacobs University Bremen, Germany
* Universite de Montreal 

## どんなもの？
ニューラルネットワークを用いた機械翻訳で，
全ての文を一度に圧縮して特徴ベクトルを作り翻訳する従来の方法から，
コンテキストにあったところから特徴ベクトルをその都度作り，翻訳する手法を提案

つまるところ，Attention Mechanismの提案

## 先行研究と比べてどこがすごい？
従来の手法(EncDec)では厳しい長文の翻訳に対しても，
提案手法は長い文章を翻訳しても精度が落ちにくかった．

## 技術や手法の肝はどこ？
ニューラルネットワークを用いた従来は全ての文を固定長にエンコード，そしてデコードしていたために，
長文に対して翻訳が困難であった．
それに対し，提案手法は出力1単語づつに，それと関連性の高いコンテキストベクターをその都度に生成する．
そのため，情報を固定長に圧縮することで欠損していた情報が，欠損なく取得できるようになった．

## どうやって有効だと検証したか？
データセットはWMT (13, 14)を用いて学習，検証を行った．

比較対象のモデルと提案モデルは30単語，50単語の長さのものを学習した．比較対象のモデルは通常のエンコーダデコーダモデル(EncDec)．

EncDecは長くなると翻訳が破綻するが，提案モデルは破綻しにくいことが分かった．
また，結果としてニューラル機械翻訳ではSOTA, 機械翻訳では同じ程度の精度がBLUEで出た．

## 議論はある？
Attention自体は精度が高くなるが，処理時間自体の違いはどうなのか．毎回計算しているため，時間掛かりそうではある．

## 次に読むべき論文は？
* Sequence to Sequence Learning with Neural Networks
