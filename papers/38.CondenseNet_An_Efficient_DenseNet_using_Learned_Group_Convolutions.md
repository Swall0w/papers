# [CondenseNet: An Efficient DenseNet using Learned Group Convolutions](https://arxiv.org/abs/1711.09224)
Huang, Gao, Liu, Shichen, van der Maaten, Laurens, Weinberger, Kilian Q.

Cornell University, Tsinghua University, Facebook AI Research

## どんなもの？(コントリビューション)
* 畳み込み時に畳み込みすべきチャネルとフィルタを意味のあるグルーピングして、計算量を減らした物体認識モデルの提案
* DenseNetや他の重いネットワークには精度として負けるところがあるが、ResNeXtよりは精度が高く、パラメータ数はオーダーが１桁違う
* モバイル系ではCIFAR-10, 100でSOTA

## 先行研究と比べてどこがすごい？
* CNNのリンクを必要のないものは接続しないようにして、計算しないところ
* パラメータ数がオーダー１桁違う
* 蒸溜を学習に組み込む手法

## 技術や手法の肝はどこ？
* 精度のいいDenseNetをベースに、計算量を減らすように学習段階で蒸溜しながらパラメータを減らす。
* 畳み込み時にすべてのチャネルについて畳み込むのではなく、グループ分けして畳み込みむことで計算回数を減らす。

## どうやって有効だと検証したか？
データセットはCIFAR-10, CIFAR-100, ImageNet
* グルーピング数を変更して、濃度を変更して、の実験
* パラメータ利用率の可視化
* 既存のモデルでの評価（error, param, FLOPs)

## 議論はある？
* learning rateの変更をsinカーブにしたのはなぜか。

## 次に読むべき論文は？
* Model Compression
* Distilling the knowledge in a neural network
* Mbolenets: Efficient convolutional neural netowrks for mobile vision applications
* Aggregated residual transformations for deep neural networks
* Model selection and estimation in regression with grouped varibales
* Interleaved group convolutions for deep neural netorks
* Shufflenet: An extremely efficient convolutional neural network for mobile devices
* Learning transferrable architectures for scalable image recognitions
