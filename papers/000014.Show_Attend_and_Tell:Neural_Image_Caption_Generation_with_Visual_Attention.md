# [Show, Attend and Tell: Neural Image Caption Generation with Visual Attention](https://arxiv.org/pdf/1502.03044.pdf)
Kelvin Xu, Jimmy Lei Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhutdinov, 
Richard S. Zemel, Yoshua Bengio

## どんなもの？
今までのイメージキャプションに加え、アテンションメカニズムを追加することで、今の文章がどこに注目されたものなのかを可視化

## 先行研究と比べてどこがすごい？
イメージキャプションにアテンションメカニズムを導入したところ

## 技術や手法の肝はどこ？
基本的なネットワークの構造はShow and Tellに似ている。

アテンションメカニズムにはhardとsoftがある。hardは見ているところそのものに着目？softは確率から着目?

## どうやって有効だと検証したか？
アテンションメカニズムの検証自体はFlicker8k, Flicker30k, COCOデータセットのBLEU-1, 2, 3, 4, METEORで数値化

他の検証として、CNNのところのモデルを変えたtらどうなるのか、シングルモデルとアンサンブルモデルの比較、データセットの分割による違いを評価

## 議論はある？
* 評価方法がちょっと古い
* アテンションメカニズムが外れた時の検討がない

## 次に読む論文は？
* Multiple object recognition with visual attention
* Neural machine translation by jointly learning to align and tanslate
* Learning phase representations using RNN encoder-decoder for statistical machine translation
* Deep visual-semantic alignments for generating image descriptions
* Sequence to sequence learning with neural networks
