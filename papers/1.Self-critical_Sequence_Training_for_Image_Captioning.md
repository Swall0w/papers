# [Self-critical Sequence Training for Image Captioning](https://arxiv.org/abs/1612.00563)
Steven J. Rennie, Etienne Marcheret, Youssef Mroueh, Jarret Ross and Vaibhava Goel

## どんなもの？
強化学習による画像キャプション

## 先行研究と比べてどこがすごい？
通常のデータセット（コンテキストが同じ）で学習したもので、
OOOCのコンテキストから外れた文章についても文章説明できる。
![img](./img/1.1.png)

## 技術や手法の肝はどこ？
以下の三つが主な技術
### Exposure biasの解決
学習時はデータセットの解答と各単語ごとに誤差逆伝搬しており、
正しい答えを入れて次の言葉を出しているが、テスト時は予測した単語がで次を答えているため誤差がたまる。

強化学習により、テスト時と訓練時を同一になるように近づける。

### Non-differentiable metric taskの解決
学習の誤差関数は評価関数を直接最適化するべき。
だけど、その評価関数は直接パラメータで偏微分出来ない。

REINFORCEのpolicy gradientで解決

### Self-critical Sequence training
このまま学習するとバッチ内の分散が激しく、安定して学習出来ない。

評価時にその時の値を誤差として使うのではなく、ベースラインとして、それまでの
モデルで最適の評価を使い、そこから±で評価
## どうやって有効だと検証したか？
クロスエントロピーと強化学習でのMSCOCOデータセットの比較等を行った。
論文投稿時SoTAとなった。
## 議論はある？
今回の結果ではImageNetで学習したCNNはFine-tuneしないでいたが、した場合には精度として上がるのか？
そもそも、強化学習からCNNまで行けるのか？

## 次に読む論文は？
Show, Attend and Tell: Neural Image Caption Generation with Visual Attention
