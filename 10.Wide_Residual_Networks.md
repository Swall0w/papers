# [Wide Residual Networks](https://arxiv.org/abs/1605.07146)
Sergey Zagoruyko, Nikos Komodakis

## どんなもの？
* resnetにおけるボトルネックアーキテクチャで幅広にした場合どう学習に影響するかを検証
* resnetにdropoutを追加
* CIFAR10, CIFAR100でSoTA
## 先行研究と比べてどこがすごい？
* 層が深くなりにつれ、特徴量が消失する現象についてどう対処していくかを実験により検証
* dropoutでresnetにおける過学習を防ぐ
* 幅が広いネットワークの有用性を提起

## 技術や手法の肝はどこ？
* ボトルネックにおける幅が広いネットワークをablation studyにより検証
## どうやって有効だと検証したか？
* CIFAR10, CIFAR100, SVHN, ImageNetで検証
* 上記3つでSoTA

## 議論はある？
* resnetは層を増やすことで性能を上げたのではない。むしろ、幅を広くし、受容野を拡張したほうが性能が上がった。
* 幅を広くすることで畳み込み計算量は増えたが、GPU上ではカーネルサイズを小さくして層を深くするより、
カーネルサイズを大きくして層を浅くしたほうが計算が特化されているため早い。
* 従って、層が深いことがいいというわけではない。

## 次に読む論文は？
* Locnet: Improving localization accuracy for object detection.
* Identity mappings in deep residual networks
* Deep networks with stochastic depth
* Highway networks
* Inception-v4, inception-resnet and the impact of residual connectioons on learning
* A multipath network for object detection
